{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118a1110",
   "metadata": {},
   "source": [
    "# Why Use torch.nn? Understanding the Benefits Over Manual PyTorch Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f3815",
   "metadata": {},
   "source": [
    "### 1. The Challenge of Creating Models Without torch.nn\n",
    "\n",
    "When you create neural networks without the torch.nn module, you have to:\n",
    "\n",
    "    Manually initialize all weights and biases with correct shapes.\n",
    "\n",
    "    Manually implement forward passes with tensor operations.\n",
    "\n",
    "    Manually compute loss functions and carefully apply clamping to avoid numerical instability.\n",
    "\n",
    "    Manually manage gradients — updating parameters and zeroing gradients every training step.\n",
    "\n",
    "    Manage complex architectures by yourself (multiple layers, activations, dropout, batchnorm, etc.).\n",
    "\n",
    "    Write a lot of boilerplate code repeatedly for common layers like linear layers, convolutions, or activation functions.\n",
    "\n",
    "    Make your code less readable and error-prone as complexity grows.\n",
    "\n",
    "    Handle device management (CPU/GPU) explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a56801",
   "metadata": {},
   "source": [
    "### 2. How Does torch.nn Help?\n",
    "\n",
    "The torch.nn module is designed to:\n",
    "\n",
    "    Provide prebuilt layer classes (nn.Linear, nn.Conv2d, nn.BatchNorm2d, etc.) that handle initialization and computation.\n",
    "\n",
    "    Automatically track parameters (weights, biases) inside nn.Module subclasses.\n",
    "\n",
    "    Define a clean forward() method for forward propagation.\n",
    "\n",
    "    Integrate seamlessly with autograd to compute gradients.\n",
    "\n",
    "    Manage model parameters easily via model.parameters().\n",
    "\n",
    "    Provide common loss functions (nn.BCELoss(), nn.CrossEntropyLoss(), etc.) that are numerically stable and optimized.\n",
    "\n",
    "    Include activation functions (nn.ReLU(), nn.Sigmoid(), nn.Softmax()) as reusable components.\n",
    "\n",
    "    Support GPU/CPU device transfer easily.\n",
    "\n",
    "    Enable modular, composable architectures by nesting nn.Modules.\n",
    "\n",
    "    Work directly with optimizers from torch.optim with parameter lists.\n",
    "\n",
    "    Simplify training loops, reducing boilerplate and errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5520f",
   "metadata": {},
   "source": [
    "### One Neuron with Sigmoid Activation\n",
    "\n",
    "Let's first create a simple model with just one linear layer followed by a sigmoid activation, using torch.nn. \n",
    "\n",
    "This is much simpler than implementing the math and gradients manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e5fdbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a909551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 1])\n",
      "Output: tensor([[0.5741],\n",
      "        [0.5385],\n",
      "        [0.5222],\n",
      "        [0.5003],\n",
      "        [0.5367],\n",
      "        [0.5410],\n",
      "        [0.5264],\n",
      "        [0.5416],\n",
      "        [0.5229],\n",
      "        [0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "Weight: Parameter containing:\n",
      "tensor([[ 0.1782,  0.2078, -0.1979, -0.1187, -0.0768]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([0.1280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # One linear layer from num_features to 1 output\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  # Linear transformation\n",
    "        x = self.sigmoid(x) # Sigmoid activation\n",
    "        return x\n",
    "\n",
    "# Create random features tensor of size (batch_size=10, num_features=5)\n",
    "features = torch.rand(10, 5)\n",
    "\n",
    "# Instantiate model\n",
    "model = SimpleModel(num_features=features.shape[1])\n",
    "\n",
    "# Forward pass\n",
    "output = model(features)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)\n",
    "print(\"Weight:\", model.linear.weight)\n",
    "print(\"Bias:\", model.linear.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e2068b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleModel                              [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 1]                   6\n",
       "├─Sigmoid: 1-2                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14647408",
   "metadata": {},
   "source": [
    "### Adding Another Layer with ReLU Activation\n",
    "\n",
    "Now, let's extend this to two linear layers with a ReLU activation in between, then a sigmoid at the end. This creates a simple 2-layer feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31fa165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 1])\n",
      "Output: tensor([[0.6656],\n",
      "        [0.7235],\n",
      "        [0.7083],\n",
      "        [0.6975],\n",
      "        [0.7161],\n",
      "        [0.7185],\n",
      "        [0.6836],\n",
      "        [0.7008],\n",
      "        [0.6939],\n",
      "        [0.7224]], grad_fn=<SigmoidBackward0>)\n",
      "linear1 Weight: Parameter containing:\n",
      "tensor([[-0.1821,  0.1225, -0.1386, -0.2808, -0.0496],\n",
      "        [ 0.1001,  0.2874,  0.2408,  0.2448,  0.3020],\n",
      "        [ 0.2126,  0.2815,  0.1191, -0.2657, -0.3535]], requires_grad=True)\n",
      "linear2 Weight: Parameter containing:\n",
      "tensor([[-0.3266,  0.4533,  0.5282]], requires_grad=True)\n",
      "linear1 Bias: Parameter containing:\n",
      "tensor([-0.0424,  0.2290,  0.4403], requires_grad=True)\n",
      "linear2 Bias: Parameter containing:\n",
      "tensor([0.2549], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerModelNoSequential(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # Define layers explicitly\n",
    "        self.linear1 = nn.Linear(num_features, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)   # First linear layer\n",
    "        x = self.relu(x)      # Activation\n",
    "        x = self.linear2(x)   # Second linear layer\n",
    "        x = self.sigmoid(x)   # Output activation\n",
    "        return x\n",
    "\n",
    "# Create dummy input\n",
    "features = torch.rand(10, 5)\n",
    "\n",
    "# Instantiate model\n",
    "model = TwoLayerModelNoSequential(num_features=features.shape[1])\n",
    "\n",
    "# Forward pass\n",
    "output = model(features)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)\n",
    "print(\"linear1 Weight:\", model.linear1.weight)\n",
    "print(\"linear2 Weight:\", model.linear2.weight)\n",
    "print(\"linear1 Bias:\", model.linear1.bias)\n",
    "print(\"linear2 Bias:\", model.linear2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da057d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TwoLayerModelNoSequential                [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 3]                   18\n",
       "├─ReLU: 1-2                              [10, 3]                   --\n",
       "├─Linear: 1-3                            [10, 1]                   4\n",
       "├─Sigmoid: 1-4                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 22\n",
       "Trainable params: 22\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e85f2",
   "metadata": {},
   "source": [
    "### Implement same model using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8d346e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 1])\n",
      "Output: tensor([[0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209],\n",
      "        [0.6209]], grad_fn=<SigmoidBackward0>)\n",
      "Parameters <bound method Module.parameters of Sequential(\n",
      "  (0): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")>\n",
      "Linear(in_features=5, out_features=3, bias=True)\n",
      "Weights: Parameter containing:\n",
      "tensor([[ 0.2097,  0.0632, -0.0453, -0.3714, -0.2241],\n",
      "        [ 0.1517, -0.3611,  0.1420, -0.3632, -0.0558],\n",
      "        [-0.3569, -0.1887,  0.1263, -0.1305, -0.3018]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([-0.2098, -0.4125, -0.2057], requires_grad=True)\n",
      "Linear(in_features=3, out_features=1, bias=True)\n",
      "Weights: Parameter containing:\n",
      "tensor([[-0.2630,  0.4056, -0.4092]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([0.4932], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),  # First layer: num_features -> 3 neurons\n",
    "            nn.ReLU(),                   # Activation function\n",
    "            nn.Linear(3, 1),             # Second layer: 3 neurons -> 1 output\n",
    "            nn.Sigmoid()                 # Sigmoid activation for output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate model\n",
    "model = TwoLayerModel(num_features=features.shape[1])\n",
    "\n",
    "# Forward pass\n",
    "output = model(features)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)\n",
    "print(\"Parameters\", model.network.parameters)\n",
    "for layer in model.network:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(layer)\n",
    "        print(\"Weights:\", layer.weight)\n",
    "        print(\"Bias:\", layer.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b5752dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TwoLayerModel                            [10, 1]                   --\n",
       "├─Sequential: 1-1                        [10, 1]                   --\n",
       "│    └─Linear: 2-1                       [10, 3]                   18\n",
       "│    └─ReLU: 2-2                         [10, 3]                   --\n",
       "│    └─Linear: 2-3                       [10, 1]                   4\n",
       "│    └─Sigmoid: 2-4                      [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 22\n",
       "Trainable params: 22\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa36af",
   "metadata": {},
   "source": [
    "### Implementation with nn.sequential, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "701141d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7613458633422852\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model with Sequential\n",
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Dummy data\n",
    "features = torch.rand(10, 5)  # 10 samples, 5 features\n",
    "labels = torch.randint(0, 2, (10, 1)).float()  # binary targets (0 or 1)\n",
    "\n",
    "# Create model\n",
    "model = TwoLayerModel(num_features=5)\n",
    "\n",
    "# Loss function: Binary Cross Entropy (for sigmoid output)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Optimizer: SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "optimizer.zero_grad()                # Zero out gradients\n",
    "outputs = model(features)            # Forward pass\n",
    "loss = loss_function(outputs, labels)    # Compute loss\n",
    "loss.backward()                      # Backpropagation\n",
    "optimizer.step()                     # Update weights\n",
    "\n",
    "# Print loss\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
