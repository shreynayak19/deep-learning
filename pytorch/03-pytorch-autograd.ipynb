{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b78033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adbc8b",
   "metadata": {},
   "source": [
    "# 1. Introduction to Autograd in PyTorch\n",
    "\n",
    "Autograd is PyTorch's automatic differentiation engine. \n",
    "\n",
    "It allows you to compute gradients automatically for tensor operations. \n",
    "\n",
    "When you set `requires_grad=True` for a tensor, PyTorch tracks all operations on it to enable automatic backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1f43f",
   "metadata": {},
   "source": [
    "# 2. Manual Derivative Example (Without PyTorch)\n",
    "\n",
    "This is a simple Python example to compute the derivative of \\( y = x^2 \\) manually using standard Python functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0692ca2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function representing dy/dx = 2x\n",
    "def dy_dx(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Compute derivative at x = 3\n",
    "dy_dx(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e552d",
   "metadata": {},
   "source": [
    "# 3. Computing Gradients Using PyTorch Autograd\n",
    "\n",
    "PyTorch can compute gradients automatically using its `autograd` engine. Let's compute the gradient of \\( y = x^2 \\) using `requires_grad=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d991440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x -> tensor(3., requires_grad=True)\n",
      "y -> tensor(9., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with gradient tracking enabled\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function y = x^2\n",
    "y = x ** 2\n",
    "\n",
    "# View tensors\n",
    "print(\"x ->\", x)\n",
    "print(\"y ->\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339918b",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "tensor(3., requires_grad=True) --> This is the tensor x.\n",
    "\n",
    "Value: 3.0\n",
    "\n",
    "### `requires_grad=True`: \n",
    "\n",
    "This means PyTorch will track operations on this tensor so that it can compute gradients later (for backpropagation).\n",
    "\n",
    "tensor(9., grad_fn=<PowBackward0>) --> This is the tensor y, which was computed as x ** 2 (i.e., 3.0 ** 2 = 9.0).\n",
    "\n",
    "Value: 9.0\n",
    "\n",
    "### `grad_fn=PowBackward0` :\n",
    "\n",
    "This tells us that y was created by an operation that supports gradient computation.\n",
    "\n",
    "Specifically, `PowBackward0` is the function that will be used during the backward pass to compute the gradient of y with respect to x (i.e., the derivative of x^2).\n",
    "\n",
    "This shows that y is part of a `computational graph` and gradients can be propagated from it.\n",
    "\n",
    "### What It Means Practically:\n",
    "\n",
    "You're setting up a small computational graph where:\n",
    "\n",
    "x is the input with gradients enabled.\n",
    "\n",
    "y = x² is the output, and PyTorch remembers how it was computed.\n",
    "\n",
    "If you later call y.backward(), PyTorch will compute dy/dx = 2x and store it in x.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d58893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backpropagate to compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Access the computed gradient (dy/dx = 2x)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475464c",
   "metadata": {},
   "source": [
    "# 4. Gradient Through a Nonlinear Function: sin(x^2)\n",
    "\n",
    "We compute the derivative of $z = sin(x^2)$ using both a manual function and PyTorch's autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63563c58",
   "metadata": {},
   "source": [
    "Manual Derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650ac3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Manual derivative: dz/dx = 2x * cos(x^2)\n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x ** 2)\n",
    "\n",
    "# Evaluate at x = 4\n",
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff56718",
   "metadata": {},
   "source": [
    "Pytorch - Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f09a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x -> tensor(4., requires_grad=True)\n",
      "y -> tensor(16., grad_fn=<PowBackward0>)\n",
      "z -> tensor(-0.2879, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch approach\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = torch.sin(y)\n",
    "\n",
    "# View tensors\n",
    "print(\"x ->\", x)\n",
    "print(\"y ->\", y)\n",
    "print(\"z ->\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dde505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd65f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dz/dx as computed by autograd\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d632bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/x2l51s4j30j612942t1zm5gm0000gq/T/ipykernel_63518/1101978012.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "# y.grad will be None (it's not a leaf node)\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ead3e5",
   "metadata": {},
   "source": [
    "### Warning message:-\n",
    "\n",
    "You're trying to access .grad on a non-leaf tensor (y).\n",
    "\n",
    "PyTorch didn’t store the gradient for it, so y.grad is None.\n",
    "\n",
    "If you really need it, you should call y.retain_grad() before backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cac12bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9577)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch approach\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# In order to retain y, we need retain_grad()\n",
    "y.retain_grad()     \n",
    "z = torch.sin(y)\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Observing y.grad\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfdd1b",
   "metadata": {},
   "source": [
    "# 5. Manual Gradient Calculation - Fw Pass & Bw Pass on Single Neuron NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff705c5",
   "metadata": {},
   "source": [
    "### 5.1. Setup Inputs and Parameters\n",
    "\n",
    "You start with a single input feature (x = 6.7) and its true label (y = 0).\n",
    "\n",
    "You initialize model parameters — weight (w) and bias (b) — without gradient tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9482559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and targets\n",
    "x = torch.tensor(6.7)  # Input feature\n",
    "y = torch.tensor(0.0)  # Ground truth label\n",
    "\n",
    "# Parameters (no gradients for now)\n",
    "w = torch.tensor(1.0)\n",
    "b = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346f078",
   "metadata": {},
   "source": [
    "### 5.2. Define Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "$loss = -(y*log (y_{pred}) + (1 - y) *log (1 - y_{pred}))$\n",
    "\n",
    "Implement the BCE loss function manually, which measures how close the model’s prediction is to the true label.\n",
    "\n",
    "The loss penalizes incorrect predictions more heavily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Binary Cross-Entropy Loss Function\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a61e3c",
   "metadata": {},
   "source": [
    "### 5.3 Forward Pass (Prediction)\n",
    "\n",
    "Compute a linear combination: \n",
    "\n",
    "$z=(w×x)+b$\n",
    "\n",
    "Apply the sigmoid function to squash  $z$ into a probability $y_{pred}$ between 0 and 1.\n",
    "\n",
    "$ y_{pred} = \\sigma(z) = 1/(1+e ^ {-z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "819d078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f208805",
   "metadata": {},
   "source": [
    "### 5.4 Calculate Loss\n",
    "\n",
    "Use the BCE loss function to compute how well the prediction matches the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "811c5a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb71fb8",
   "metadata": {},
   "source": [
    "### 5.5 Manually Compute Gradients\n",
    "\n",
    "Calculate how the loss changes with respect to the prediction $(dloss/dy_{pred})$.\n",
    "\n",
    "Calculate how the prediction changes with respect to the linear output $(dy_{pred}/dz)$.\n",
    "\n",
    "Calculate how the linear output changes with respect to the parameters $(dz/dw, dz/db)$.\n",
    "\n",
    "Use the chain rule to combine these derivatives and get gradients of loss with respect to $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9342122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "# Manual gradients\n",
    "dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "\n",
    "# Final gradients\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db\n",
    "\n",
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc0a67",
   "metadata": {},
   "source": [
    "# 6. Pytorch Autograd Gradient Calculation - Fw Pass & Bw Pass on Single Neuron NN\n",
    "\n",
    "Now we use PyTorch autograd to compute the same gradients automatically for weight and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c725bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Autograd Gradient w.r.t weight tensor(6.6918)\n",
      "Pytorch Autograd Gradient w.r.t bias tensor(0.9988)\n",
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "# Inputs and targets\n",
    "x = torch.tensor(6.7)  # Input feature\n",
    "y = torch.tensor(0.0)  # Ground truth label\n",
    "\n",
    "# Enable gradient tracking\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Linear transformation and sigmoid\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "\n",
    "# Loss computation\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# View gradients\n",
    "print(\"Pytorch Autograd Gradient w.r.t weight\", w.grad)  # Should match manual dw\n",
    "print(\"Pytorch Autograd Gradient w.r.t bias\", b.grad)  # Should match manual db\n",
    "\n",
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1d015",
   "metadata": {},
   "source": [
    "# 7. Gradient of Mean Squared Function\n",
    "\n",
    "Compute gradient of the mean of squared elements in a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0437b88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Function: mean of squared elements\n",
    "y = (x ** 2).mean()\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Gradients of y w.r.t x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f8ec5",
   "metadata": {},
   "source": [
    "# 8. Clearing Gradients in PyTorch\n",
    "\n",
    "Always clear old gradients before performing another backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2aa5ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x 4.0\n",
      "gradient of x 0.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "\n",
    "# View gradient\n",
    "print(f\"gradient of x {x.grad}\")\n",
    "\n",
    "# Clear the gradient manually\n",
    "print(f\"gradient of x {x.grad.zero_()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029bc60",
   "metadata": {},
   "source": [
    "# 9. Disabling Gradient Tracking\n",
    "\n",
    "Use different methods to disable gradient tracking in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e575f",
   "metadata": {},
   "source": [
    "Option 1: requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cbe1a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x**2 = without grad_fn  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: requires_grad_(False)\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x.requires_grad_(False)\n",
    "\n",
    "# No gradient tracking here\n",
    "y = x ** 2\n",
    "print(\"y = x**2 = without grad_fn \", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e984464",
   "metadata": {},
   "source": [
    "Option 2: detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5efb0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor(2., requires_grad=True)\n",
      "y = x**2 = with grad_fn  tensor(4., grad_fn=<PowBackward0>)\n",
      "y1 = z**2 = without grad_fn  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Option 2: detach()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(\"x\", x)\n",
    "z = x.detach()  # Detach from computation graph\n",
    "\n",
    "# Computation on x\n",
    "y = x ** 2\n",
    "print(\"y = x**2 = with grad_fn \", y)\n",
    "\n",
    "# Computation on z will not be tracked\n",
    "y1 = z ** 2\n",
    "print(\"y1 = z**2 = without grad_fn \", y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee5566",
   "metadata": {},
   "source": [
    "Option 3: with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d35cc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = x**2 = without grad_fn  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Option 3: with torch.no_grad()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    print(\"y = x**2 = without grad_fn \", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d5a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
